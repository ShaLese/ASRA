{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analyzer Agent\n",
    "\n",
    "This notebook implements the data analysis component using TabNet for structured data analysis of experimental results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Dict, Tuple\n",
    "import torch\n",
    "from utils.config import setup_logging, MODEL_CONFIGS, EXPERIMENTAL_DATA_DIR, OUTPUTS_DIR\n",
    "from utils.helpers import save_json, save_dataframe\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Setup logging\n",
    "logger = setup_logging('data_analyzer')\n",
    "\n",
    "class DataAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tabnet_params = MODEL_CONFIGS['data_analysis']['tabnet_params']\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def prepare_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Prepare data for TabNet analysis.\"\"\"\n",
    "        # Identify numeric columns\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        # Scale numeric features\n",
    "        X = self.scaler.fit_transform(df[numeric_cols])\n",
    "        \n",
    "        # Use last column as target variable\n",
    "        y = X[:, -1]\n",
    "        X = X[:, :-1]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def analyze_dataset(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Analyze dataset using TabNet.\"\"\"\n",
    "        X, y = self.prepare_data(df)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Initialize and train TabNet\n",
    "        model = TabNetRegressor(**self.tabnet_params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            max_epochs=100,\n",
    "            patience=10,\n",
    "            batch_size=32\n",
    "        )\n",
    "        \n",
    "        # Get feature importance\n",
    "        feature_importance = model.feature_importances_\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        return {\n",
    "            'feature_importance': feature_importance.tolist(),\n",
    "            'predictions': y_pred.tolist(),\n",
    "            'actual_values': y_test.tolist(),\n",
    "            'model_params': self.tabnet_params\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_experimental_data() -> Dict:\n",
    "    \"\"\"Analyze all experimental data files.\"\"\"\n",
    "    logger.info('Starting experimental data analysis')\n",
    "    \n",
    "    analyzer = DataAnalyzer()\n",
    "    analysis_results = {}\n",
    "    \n",
    "    # Process all CSV files in experimental data directory\n",
    "    for data_file in tqdm(list(EXPERIMENTAL_DATA_DIR.glob('*.csv')), desc='Analyzing datasets'):\n",
    "        try:\n",
    "            # Load dataset\n",
    "            df = pd.read_csv(data_file)\n",
    "            \n",
    "            # Analyze dataset\n",
    "            results = analyzer.analyze_dataset(df)\n",
    "            \n",
    "            # Store results\n",
    "            analysis_results[data_file.stem] = results\n",
    "            logger.info(f'Successfully analyzed dataset: {data_file.name}')\n",
    "            \n",
    "            # Save individual results\n",
    "            output_path = OUTPUTS_DIR / f'analysis_{data_file.stem}.json'\n",
    "            save_json(results, output_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f'Error analyzing dataset {data_file.name}: {str(e)}')\n",
    "    \n",
    "    # Save combined results\n",
    "    output_path = OUTPUTS_DIR / 'experimental_analysis.json'\n",
    "    save_json(analysis_results, output_path)\n",
    "    logger.info(f'Saved combined analysis results to {output_path}')\n",
    "    \n",
    "    return analysis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Run analysis\n",
    "    results = analyze_experimental_data()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Analyzed {len(results)} datasets\")\n",
    "    for dataset_name, analysis in results.items():\n",
    "        print(f\"\\nDataset: {dataset_name}\")\n",
    "        print(f\"Feature Importance:\")\n",
    "        for i, importance in enumerate(analysis['feature_importance']):\n",
    "            print(f\"Feature {i}: {importance:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
