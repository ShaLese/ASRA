{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature Review Agent\n",
    "\n",
    "This notebook implements the literature review component using BERT and SPECTER models for semantic analysis of research papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from utils.config import setup_logging, MODEL_CONFIGS, RESEARCH_PAPERS_DIR, OUTPUTS_DIR\n",
    "from utils.helpers import load_research_papers, save_json\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Setup logging\n",
    "logger = setup_logging('literature_review')\n",
    "\n",
    "# Load models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# BERT for text analysis\n",
    "bert_name = MODEL_CONFIGS['literature_review']['bert_model']\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_name)\n",
    "bert_model = AutoModel.from_pretrained(bert_name).to(device)\n",
    "\n",
    "# SPECTER for citation analysis\n",
    "specter_name = MODEL_CONFIGS['literature_review']['specter_model']\n",
    "specter_tokenizer = AutoTokenizer.from_pretrained(specter_name)\n",
    "specter_model = AutoModel.from_pretrained(specter_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def extract_key_insights(text: str, max_length: int = 512) -> Dict:\n",
    "    \"\"\"Extract key insights from text using BERT.\"\"\"\n",
    "    # Tokenize and encode text\n",
    "    inputs = bert_tokenizer(text, return_tensors='pt', max_length=max_length,\n",
    "                           truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    # Extract key sentences (simplified for demonstration)\n",
    "    sentences = text.split('.')\n",
    "    sentence_scores = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(sentence.strip()) > 0:\n",
    "            inputs = bert_tokenizer(sentence, return_tensors='pt', max_length=128,\n",
    "                                   truncation=True, padding=True)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = bert_model(**inputs)\n",
    "                sentence_emb = outputs.last_hidden_state.mean(dim=1)\n",
    "                score = torch.cosine_similarity(embeddings, sentence_emb)\n",
    "                sentence_scores.append((sentence.strip(), score.item()))\n",
    "    \n",
    "    # Sort sentences by importance score\n",
    "    sentence_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return {\n",
    "        'key_points': [s[0] for s in sentence_scores[:3]],\n",
    "        'importance_scores': [s[1] for s in sentence_scores[:3]]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_papers() -> Dict:\n",
    "    \"\"\"Analyze all papers in the research directory.\"\"\"\n",
    "    logger.info('Starting paper analysis')\n",
    "    \n",
    "    # Load papers\n",
    "    papers = load_research_papers(RESEARCH_PAPERS_DIR)\n",
    "    if not papers:\n",
    "        logger.warning('No papers found in research directory')\n",
    "        return {}\n",
    "    \n",
    "    analysis_results = {}\n",
    "    for paper in tqdm(papers, desc='Analyzing papers'):\n",
    "        try:\n",
    "            # Extract insights\n",
    "            insights = extract_key_insights(paper['content'])\n",
    "            \n",
    "            # Store results\n",
    "            analysis_results[paper['title']] = {\n",
    "                'key_insights': insights['key_points'],\n",
    "                'importance_scores': insights['importance_scores']\n",
    "            }\n",
    "            \n",
    "            logger.info(f'Successfully analyzed paper: {paper[\"title\"]}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f'Error analyzing paper {paper[\"title\"]}: {str(e)}')\n",
    "    \n",
    "    # Save results\n",
    "    output_path = OUTPUTS_DIR / 'literature_analysis.json'\n",
    "    save_json(analysis_results, output_path)\n",
    "    logger.info(f'Saved analysis results to {output_path}')\n",
    "    \n",
    "    return analysis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Run analysis\n",
    "    results = analyze_papers()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Analyzed {len(results)} papers\")\n",
    "    for title, analysis in results.items():\n",
    "        print(f\"\\n{title}:\")\n",
    "        for i, (insight, score) in enumerate(zip(analysis['key_insights'], \n",
    "                                                analysis['importance_scores']), 1):\n",
    "            print(f\"{i}. {insight} (score: {score:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
